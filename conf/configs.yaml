datasets:
  librispeech:
    clean_path: /data_disk/zlf/dataloder/LibriSpeech/
    other_path: /data_disk/zlf/dataloder/LibriSpeech/
    n_fft: 400
    train: train100
    val: dev
    test: test_module

  aishell:
    manifest_path: /data_disk/zlf/code/jModel/conformer-rnnt/manifest/

  dataset_selected: aishell

text_process:
  lang: zh
  word_dict_path: /data_disk/zlf/code/jModel/conformer-rnnt/word_dict.txt

datamodule:
  aishell:
    batch_size: ${training.batch_size}
    num_workers: 0

  librispeech:
    batch_size: ${training.batch_size}
    num_workers: 0

model:
  num_classes: ??
  input_dim: 80
  encoder_dim: 512
  num_encoder_layers: 12
  num_attention_heads: 8
  feed_forward_expansion_factor: 4
  conv_expansion_factor: 2
  input_dropout_p: 0.1
  feed_forward_dropout_p: 0.1
  attention_dropout_p:  0.1
  conv_dropout_p: 0.1
  conv_kernel_size: 31
  half_step_residual: True

  decoder_output_dim: ${model.encoder_dim}
  hidden_state_dim: 320
  decoder_num_layers: 1

  freq_masks: 2
  time_masks: 10
  freq_width: 27
  time_width: 0.05

  rnn_type: lstm
  sos_id: 1
  eos_id: 2

  grad_ckpt_batchsize: 4

training:
  batch_size: 24
  max_epoch: 50

optimizer:
  optimizer_name: adam
  betas: [ 0.9, 0.98 ]
  weight_decay: 1e-3


lr_scheduler:
  lr: 0.001
#  scheduler_name: None
#  gamma: 0.1
  last_epoch: ${training.max_epoch}

  num_warmup_steps: 4000
  num_training_steps: 100000

tb_logger:
  save_dir: tb_logs
  name: conformer_logs

trainer:
  max_epochs: ${training.max_epoch}
  enable_progress_bar: True
  accelerator: auto
  detect_anomaly: True
  accumulate_grad_batches: 8
  gradient_clip_val: 5.0


ckpt:
  have_ckpt: False
  ckpt_path: "/data_disk/zlf/code/jModel/conformer-rnnt//checkpoint/conformer_rnnt.ckpt"
  save_path: "/data_disk/zlf/code/jModel/conformer-rnnt//checkpoint/conformer_rnnt_common.ckpt"
  train: True
