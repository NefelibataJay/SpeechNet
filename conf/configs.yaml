datamodule:
  dataset:
    dataset_path: E:/datasets/LibriSpeech
    feature_types: fbank
    num_mel_bins: 80
    sample_rate: 16000

  #    spec_agument: True
  #    noise_augment: True

  batch_size: ${training.batch_size}
  one_dataset: False
  val_set_ratio: 0.05
  test_set_ratio: 0.05
  num_workers: 0
  manifest_path: D:/model/rnn-t/libriSpeech_manifest/

tokenizer:
  lang: zh
  word_dict_path: D:\model\rnn-t\manifest\vocab.txt

logger:
  root_dir: tb_logs
  name: model_logs

model:
  num_classes: ??
  input_dim: 80 # 80 for fbank
  encoder_dim: 512
  num_encoder_layers: 12
  num_attention_heads: 8
  feed_forward_expansion_factor: 4
  conv_expansion_factor: 2
  input_dropout_p: 0.1
  feed_forward_dropout_p: 0.1
  attention_dropout_p: 0.1
  conv_dropout_p: 0.1
  conv_kernel_size: 31
  half_step_residual: True

  decoder_output_dim: ${model.encoder_dim}
  hidden_state_dim: 320
  decoder_num_layers: 1

  freq_masks: 2
  time_masks: 10
  freq_width: 27
  time_width: 0.05

  rnn_type: lstm
  sos_id: 1
  eos_id: 2

  grad_ckpt_batchsize: 4


training:
  batch_size: 24
  max_epoch: 50

optimizer:
  optimizer_name: adam
  betas: [ 0.9, 0.98 ]
  weight_decay: 1e-3


lr_scheduler:
  lr: 0.001
  #  scheduler_name: None
  #  gamma: 0.1
  last_epoch: ${training.max_epoch}


trainer:
  max_epochs: ${training.max_epoch}
  enable_progress_bar: True
  accelerator: auto
  detect_anomaly: True
  accumulate_grad_batches: 8
  gradient_clip_val: 5.0

ckpt:
  have_ckpt: False
  ckpt_path: "/data_disk/zlf/code/jModel/conformer-rnnt//checkpoint/conformer_rnnt.ckpt"
  save_path: "/data_disk/zlf/code/jModel/conformer-rnnt//checkpoint/conformer_rnnt_common.ckpt"
  train: True
