{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:07:35.854615600Z",
     "start_time": "2023-06-06T08:07:32.240933200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 size: int,\n",
    "                 smoothing: float = 0.1,\n",
    "                 padding_idx: int = 0,\n",
    "                 normalize_length: bool = False):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"none\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.normalize_length = normalize_length\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch, seqlen, class)  (batch, seqlen)\n",
    "        assert x.size(2) == self.size\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(-1, self.size)\n",
    "        target = target.view(-1)\n",
    "        # use zeros_like instead of torch.no_grad() for true_dist,\n",
    "        # since no_grad() can not be exported by JIT\n",
    "        true_dist = torch.zeros_like(x)\n",
    "        true_dist.fill_(self.smoothing / (self.size - 1))\n",
    "        ignore = target == self.padding_idx  # (B,)\n",
    "        total = len(target) - ignore.sum().item()\n",
    "        target = target.masked_fill(ignore, 0)  # avoid -1 index\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        kl = self.criterion(torch.log_softmax(x, dim=1), true_dist)\n",
    "        denom = total if self.normalize_length else batch_size\n",
    "        return kl.masked_fill(ignore.unsqueeze(1), 0).sum() / denom\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor([[[0.4, 0.6], [0.5, 0.5]], [[0.3, 0.7], [0.8, 0.2]]])\n",
    "targets = torch.tensor([[1,2],[0,1]])\n",
    "\n",
    "loss = LabelSmoothingLoss(2, 0.1)\n",
    "loss(x,targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CELoss(nn.Module):\n",
    "    ''' Cross Entropy Loss with label smoothing '''\n",
    "\n",
    "    def __init__(self, label_smooth=None, class_num=137):\n",
    "        super().__init__()\n",
    "        self.label_smooth = label_smooth\n",
    "        self.class_num = class_num\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        '''\n",
    "        Args:\n",
    "            pred: prediction of model output    [N, M]\n",
    "            target: ground truth of sampler [N]\n",
    "        '''\n",
    "        eps = 1e-12\n",
    "\n",
    "        if self.label_smooth is not None:\n",
    "            # cross entropy loss with label smoothing\n",
    "            logprobs = F.log_softmax(pred, dim=1)  # softmax + log\n",
    "            target = F.one_hot(target, self.class_num)  # 转换成one-hot\n",
    "\n",
    "            # label smoothing\n",
    "            # 实现 1\n",
    "            # target = (1.0-self.label_smooth)*target + self.label_smooth/self.class_num\n",
    "            # 实现 2\n",
    "            # implement 2\n",
    "            target = torch.clamp(target.float(), min=self.label_smooth / (self.class_num - 1),\n",
    "                                 max=1.0 - self.label_smooth)\n",
    "            loss = -1 * torch.sum(target * logprobs, 1)\n",
    "\n",
    "        else:\n",
    "            # standard cross entropy loss\n",
    "            loss = -1. * pred.gather(1, target.unsqueeze(-1)) + torch.log(torch.exp(pred + eps).sum(dim=1))\n",
    "\n",
    "        return loss.mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = CELoss(label_smooth=None, class_num=3)\n",
    "\n",
    "x = torch.tensor([[5, 6, 7], [4, 4, 4]], dtype=torch.float)\n",
    "y = torch.tensor([1, 2])\n",
    "\n",
    "print(loss1(x, y), loss2(x, y))\n",
    "# tensor(0.0018) tensor(0.0018)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = CELoss(label_smooth=0.05, class_num=3)\n",
    "\n",
    "x = torch.tensor([[1, 8, 1], [1, 1, 8]], dtype=torch.float)\n",
    "y = torch.tensor([1, 2])\n",
    "\n",
    "print(loss1(x, y), loss2(x, y))\n",
    "# tensor(0.0018) tensor(0.2352)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.tensor([[[2, 3], [4, 5]], [[8, 9], [10, 11]]])\n",
    "targets = torch.tensor([1,2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = LabelSmoothingLoss(2, 0.1)\n",
    "loss(x,targets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-1.6336, -1.6944, -1.0283, -3.1764, -1.5064],\n         [-1.6933, -0.8209, -2.0381, -3.3705, -1.5540],\n         [-2.8446, -0.5310, -2.8125, -3.5543, -1.3274]]),\n tensor([0, 2, 3]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(3,5).log_softmax(dim=-1)\n",
    "targets = torch.tensor([0,2,3])\n",
    "\n",
    "logits,targets\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:17:32.806738300Z",
     "start_time": "2023-06-06T08:17:32.777036500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2.5357)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits,targets,ignore_index=2,label_smoothing=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T08:17:53.190758600Z",
     "start_time": "2023-06-06T08:17:53.167729600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
